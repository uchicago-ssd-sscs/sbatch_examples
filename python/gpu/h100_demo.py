#!/usr/bin/env python3

import torch
import time
import numpy as np
import psutil
import platform

# Global storage for timing results
gpu_times = {}
cpu_times = {}

def print_header():
    print("=" * 80)
    print("üöÄ H100 GPU POWER DEMONSTRATION")
    print("=" * 80)
    print(f"Hostname: {platform.node()}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print(f"CPU Cores: {psutil.cpu_count()}")
    print(f"System Memory: {psutil.virtual_memory().total / 1e9:.1f} GB")
    print("=" * 80)

def run_gpu_matrix_ops():
    """Run all matrix operations on GPU first"""
    print("\nüî• PHASE 1: GPU MATRIX OPERATIONS")
    print("-" * 50)
    
    sizes = [8192, 16384, 32768]
    
    for size in sizes:
        print(f"\nüìä GPU: {size:,} √ó {size:,} matrix multiplication")
        print(f"   Matrix size: {size * size * 4 / 1e9:.1f} GB")
        
        # GPU test
        print("   üöÄ GPU calculation...")
        a_gpu = torch.randn(size, size, dtype=torch.float32, device='cuda')
        b_gpu = torch.randn(size, size, dtype=torch.float32, device='cuda')
        
        torch.cuda.synchronize()
        start = time.time()
        c_gpu = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        gpu_gflops = (2 * size**3) / gpu_time / 1e9
        
        gpu_times[f'matrix_{size}'] = gpu_time
        
        print(f"   ‚úÖ GPU: {gpu_time:.2f}s ({gpu_gflops:.0f} GFLOPS)")
        
        if size == 32768:
            print(f"   üèÜ H100 processed a {size:,}√ó{size:,} matrix in {gpu_time:.2f} seconds!")
            print(f"   üèÜ That's {gpu_gflops:.0f} billion floating-point operations per second!")

def run_gpu_memory_bandwidth():
    """Run memory bandwidth tests on GPU"""
    print("\nüî• PHASE 2: GPU MEMORY BANDWIDTH")
    print("-" * 50)
    
    sizes_gb = [1, 4, 8, 16]
    
    for size_gb in sizes_gb:
        size_elements = int(size_gb * 1024**3 // 4)  # float32 = 4 bytes
        print(f"\nüìä GPU: {size_gb}GB memory transfer")
        
        # Create data
        data_cpu = torch.randn(size_elements, dtype=torch.float32)
        
        # H2D transfer
        torch.cuda.synchronize()
        start = time.time()
        data_gpu = data_cpu.cuda()
        torch.cuda.synchronize()
        h2d_time = time.time() - start
        h2d_bandwidth = size_gb / h2d_time
        
        # D2H transfer
        torch.cuda.synchronize()
        start = time.time()
        data_cpu_back = data_gpu.cpu()
        torch.cuda.synchronize()
        d2h_time = time.time() - start
        d2h_bandwidth = size_gb / d2h_time
        
        gpu_times[f'memory_{size_gb}gb_h2d'] = h2d_time
        gpu_times[f'memory_{size_gb}gb_d2h'] = d2h_time
        
        print(f"   üöÄ H2D: {h2d_time:.2f}s ({h2d_bandwidth:.1f} GB/s)")
        print(f"   üöÄ D2H: {d2h_time:.2f}s ({d2h_bandwidth:.1f} GB/s)")
        
        if size_gb == 16:
            print(f"   üèÜ H100 transferred 16GB in {h2d_time:.2f} seconds!")
            print(f"   üèÜ That's {h2d_bandwidth:.1f} GB/s bandwidth!")

def run_gpu_parallel_processing():
    """Run parallel processing tests on GPU"""
    print("\nüî• PHASE 3: GPU PARALLEL PROCESSING")
    print("-" * 50)
    
    sizes = [10_000_000, 50_000_000, 100_000_000]
    
    for size in sizes:
        print(f"\nüìä GPU: {size:,} element parallel computation")
        
        # GPU test
        print("   üöÄ GPU processing...")
        a_gpu = torch.randn(size, dtype=torch.float32, device='cuda')
        b_gpu = torch.randn(size, dtype=torch.float32, device='cuda')
        
        torch.cuda.synchronize()
        start = time.time()
        c_gpu = torch.sin(a_gpu) * torch.cos(b_gpu) + torch.sqrt(torch.abs(a_gpu))
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        
        gpu_times[f'parallel_{size}'] = gpu_time
        
        print(f"   ‚úÖ GPU: {gpu_time:.2f}s")
        
        if size == 100_000_000:
            print(f"   üèÜ H100 processed 100 million elements in {gpu_time:.2f} seconds!")

def run_gpu_monte_carlo():
    """Run Monte Carlo econometrics on GPU"""
    print("\nüî• PHASE 4: GPU MONTE CARLO ECONOMETRICS")
    print("-" * 50)
    
    print("\nüìä GPU: Large-Scale Monte Carlo Studies for Economics Research")
    print("   Simulating: Asset Pricing Models, VAR Analysis, and Bootstrap Methods")
    
    # Monte Carlo parameters
    n_simulations = [10000, 50000, 100000]
    n_assets = 100
    n_time_periods = 252  # One trading year
    
    for n_sim in n_simulations:
        print(f"\nüéØ GPU Monte Carlo Study: {n_sim:,} simulations")
        print(f"   Assets: {n_assets}, Time periods: {n_time_periods}")
        print(f"   Total calculations: {n_sim * n_assets * n_time_periods:,}")
        
        # GPU Monte Carlo - Asset Price Simulation
        print("   üöÄ GPU: Simulating asset price paths...")
        torch.cuda.synchronize()
        start = time.time()
        
        # Simulate on GPU
        gpu_returns = torch.normal(0.0001, 0.02, (n_sim, n_assets, n_time_periods), device='cuda')
        gpu_prices = torch.cumprod(1 + gpu_returns, dim=2)
        
        # Calculate portfolio statistics on GPU
        gpu_portfolio_values = torch.sum(gpu_prices, dim=1)
        gpu_var_95 = torch.quantile(gpu_portfolio_values, 0.05)
        gpu_expected_return = torch.mean(gpu_portfolio_values)
        
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        
        gpu_times[f'monte_carlo_{n_sim}'] = gpu_time
        
        print(f"   ‚úÖ GPU: {gpu_time:.2f}s (VaR: ${gpu_var_95:.2f}, E[R]: {gpu_expected_return:.4f})")
        
        if n_sim == 100000:
            print(f"   üèÜ H100 completed 100K Monte Carlo simulations in {gpu_time:.2f} seconds!")
            print(f"   üèÜ That's {n_sim * n_assets * n_time_periods / gpu_time / 1e6:.1f}M calculations per second!")
    
    # Bootstrap Confidence Intervals Demo
    print(f"\nüìä GPU Bootstrap Confidence Intervals (10,000 resamples)")
    print("   Simulating: Regression coefficient confidence intervals")
    
    # Generate synthetic economic data
    n_obs = 10000
    x = torch.randn(n_obs, 5, device='cuda')  # 5 explanatory variables
    true_beta = torch.tensor([1.5, -0.8, 0.3, 1.2, -0.5], device='cuda')
    noise = torch.normal(0, 0.1, (n_obs,), device='cuda')
    y = torch.matmul(x, true_beta) + noise
    
    # Bootstrap regression coefficients
    n_bootstrap = 10000
    bootstrap_betas = torch.zeros(n_bootstrap, 5, device='cuda')
    
    torch.cuda.synchronize()
    start = time.time()
    
    for i in range(n_bootstrap):
        # Bootstrap sample
        indices = torch.randint(0, n_obs, (n_obs,), device='cuda')
        x_boot = x[indices]
        y_boot = y[indices]
        
        # OLS regression: (X'X)^(-1) X'y
        xtx = torch.matmul(x_boot.t(), x_boot)
        xty = torch.matmul(x_boot.t(), y_boot)
        beta_hat = torch.linalg.solve(xtx, xty)
        
        bootstrap_betas[i] = beta_hat
    
    torch.cuda.synchronize()
    bootstrap_time = time.time() - start
    
    gpu_times['bootstrap_10000'] = bootstrap_time
    
    # Calculate confidence intervals
    ci_lower = torch.quantile(bootstrap_betas, 0.025, dim=0)
    ci_upper = torch.quantile(bootstrap_betas, 0.975, dim=0)
    
    print(f"   ‚úÖ GPU Bootstrap completed in {bootstrap_time:.2f} seconds")
    print(f"   üìà 95% Confidence Intervals:")
    for i in range(5):
        print(f"      Œ≤{i+1}: [{ci_lower[i]:.3f}, {ci_upper[i]:.3f}] (true: {true_beta[i]:.3f})")
    
    print(f"   üèÜ H100 performed {n_bootstrap} bootstrap regressions in {bootstrap_time:.2f} seconds!")

def run_gpu_social_science_simulations():
    """Run social science simulations on GPU"""
    print("\nüî• PHASE 4.5: GPU SOCIAL SCIENCE SIMULATIONS")
    print("-" * 50)
    
    print("\nüìä GPU: Large-Scale Social Science Research Simulations")
    print("   Simulating: Agent-Based Models, Network Analysis, Survey Research")
    
    # Agent-Based Model: Social Contagion Simulation
    print("\nüß† AGENT-BASED MODEL: Social Contagion Simulation")
    print("   Simulating: Information spread in social networks")
    
    n_agents = [10000, 50000, 100000]
    n_iterations = 100
    
    for n_agent in n_agents:
        print(f"\nüéØ GPU Agent-Based Model: {n_agent:,} agents, {n_iterations} iterations")
        print(f"   Total interactions: {n_agent * n_iterations:,}")
        
        torch.cuda.synchronize()
        start = time.time()
        
        # Initialize agent states (0 = unaware, 1 = aware)
        agent_states = torch.zeros(n_agent, dtype=torch.float32, device='cuda')
        agent_states[0] = 1.0  # Start with one informed agent
        
        # Social network connectivity (random connections)
        connectivity = torch.rand(n_agent, n_agent, device='cuda') < 0.01  # 1% connection probability
        connectivity = connectivity.float()
        
        # Contagion simulation
        for iteration in range(n_iterations):
            # Calculate influence from neighbors
            influence = torch.matmul(connectivity, agent_states)
            # Update states based on influence threshold
            new_states = torch.where(influence > 0.5, 1.0, agent_states)
            agent_states = new_states
        
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        
        final_adoption = torch.sum(agent_states).item()
        adoption_rate = final_adoption / n_agent * 100
        
        gpu_times[f'agent_based_{n_agent}'] = gpu_time
        
        print(f"   ‚úÖ GPU: {gpu_time:.2f}s (Final adoption: {adoption_rate:.1f}%)")
        
        if n_agent == 100000:
            print(f"   üèÜ H100 simulated 100K agents in {gpu_time:.2f} seconds!")
            print(f"   üèÜ That's {n_agent * n_iterations / gpu_time / 1e6:.1f}M interactions per second!")
    
    # Network Analysis: Community Detection
    print("\nüåê NETWORK ANALYSIS: Community Detection")
    print("   Simulating: Large-scale social network analysis")
    
    n_nodes = [5000, 10000, 20000]
    
    for n_node in n_nodes:
        print(f"\nüéØ GPU Network Analysis: {n_node:,} nodes")
        print(f"   Potential connections: {n_node * (n_node - 1) // 2:,}")
        
        torch.cuda.synchronize()
        start = time.time()
        
        # Generate random network (sparse)
        edge_prob = 0.001  # Sparse network
        adjacency = torch.rand(n_node, n_node, device='cuda') < edge_prob
        adjacency = adjacency.float()
        adjacency = torch.triu(adjacency, diagonal=1) + torch.triu(adjacency, diagonal=1).t()  # Make symmetric
        
        # Calculate network metrics
        degree_centrality = torch.sum(adjacency, dim=1)
        clustering_coeff = torch.zeros(n_node, device='cuda')
        
        # Calculate clustering coefficient for each node
        for i in range(n_node):
            neighbors = torch.where(adjacency[i] > 0)[0]
            if len(neighbors) > 1:
                neighbor_connections = adjacency[neighbors][:, neighbors]
                triangles = torch.sum(neighbor_connections) / 2
                possible_triangles = len(neighbors) * (len(neighbors) - 1) / 2
                if possible_triangles > 0:
                    clustering_coeff[i] = triangles / possible_triangles
        
        avg_clustering = torch.mean(clustering_coeff)
        avg_degree = torch.mean(degree_centrality)
        
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        
        gpu_times[f'network_{n_node}'] = gpu_time
        
        print(f"   ‚úÖ GPU: {gpu_time:.2f}s (Avg degree: {avg_degree:.1f}, Clustering: {avg_clustering:.3f})")
        
        if n_node == 20000:
            print(f"   üèÜ H100 analyzed 20K node network in {gpu_time:.2f} seconds!")
    
    # Survey Research: Large-Scale Factor Analysis
    print("\nüìä SURVEY RESEARCH: Large-Scale Factor Analysis")
    print("   Simulating: Psychological scale validation")
    
    n_respondents = [10000, 50000, 100000]
    n_items = 50
    n_factors = 5
    
    for n_resp in n_respondents:
        print(f"\nüéØ GPU Factor Analysis: {n_resp:,} respondents, {n_items} items, {n_factors} factors")
        print(f"   Data points: {n_resp * n_items:,}")
        
        torch.cuda.synchronize()
        start = time.time()
        
        # Generate survey data (responses 1-7 Likert scale)
        survey_data = torch.randint(1, 8, (n_resp, n_items), dtype=torch.float32, device='cuda')
        
        # Standardize data
        survey_std = torch.std(survey_data, dim=0, keepdim=True)
        survey_mean = torch.mean(survey_data, dim=0, keepdim=True)
        survey_z = (survey_data - survey_mean) / survey_std
        
        # Calculate correlation matrix
        corr_matrix = torch.matmul(survey_z.t(), survey_z) / (n_resp - 1)
        
        # Simple factor analysis (eigenvalue decomposition)
        eigenvalues, eigenvectors = torch.linalg.eigh(corr_matrix)
        
        # Extract top factors
        top_eigenvalues = eigenvalues[-n_factors:]
        top_eigenvectors = eigenvectors[:, -n_factors:]
        
        # Calculate factor loadings
        factor_loadings = top_eigenvectors * torch.sqrt(top_eigenvalues.unsqueeze(0))
        
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        
        explained_variance = torch.sum(top_eigenvalues) / torch.sum(eigenvalues) * 100
        
        gpu_times[f'factor_analysis_{n_resp}'] = gpu_time
        
        print(f"   ‚úÖ GPU: {gpu_time:.2f}s (Explained variance: {explained_variance:.1f}%)")
        
        if n_resp == 100000:
            print(f"   üèÜ H100 analyzed 100K respondent survey in {gpu_time:.2f} seconds!")
    
    # Political Science: Voting Behavior Simulation
    print("\nüó≥Ô∏è POLITICAL SCIENCE: Voting Behavior Simulation")
    print("   Simulating: Electoral dynamics and voter preferences")
    
    n_voters = [50000, 100000, 200000]
    n_candidates = 5
    n_issues = 10
    
    for n_voter in n_voters:
        print(f"\nüéØ GPU Voting Simulation: {n_voter:,} voters, {n_candidates} candidates, {n_issues} issues")
        print(f"   Preference calculations: {n_voter * n_candidates * n_issues:,}")
        
        torch.cuda.synchronize()
        start = time.time()
        
        # Generate voter preferences on issues (1-10 scale)
        voter_preferences = torch.rand(n_voter, n_issues, device='cuda') * 10
        
        # Generate candidate positions on issues
        candidate_positions = torch.rand(n_candidates, n_issues, device='cuda') * 10
        
        # Calculate preference distances
        distances = torch.zeros(n_voter, n_candidates, device='cuda')
        for i in range(n_candidates):
            diff = voter_preferences - candidate_positions[i].unsqueeze(0)
            distances[:, i] = torch.sqrt(torch.sum(diff**2, dim=1))
        
        # Convert distances to preferences (lower distance = higher preference)
        preferences = 1.0 / (1.0 + distances)
        
        # Simulate voting (proportional to preferences)
        vote_probabilities = preferences / torch.sum(preferences, dim=1, keepdim=True)
        votes = torch.multinomial(vote_probabilities, 1).squeeze()
        
        # Calculate vote shares
        vote_counts = torch.bincount(votes, minlength=n_candidates)
        vote_shares = vote_counts / n_voter * 100
        
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        
        gpu_times[f'voting_{n_voter}'] = gpu_time
        
        print(f"   ‚úÖ GPU: {gpu_time:.2f}s (Winner: Candidate {torch.argmax(vote_shares).item()}, {vote_shares[torch.argmax(vote_shares)].item():.1f}%)")
        
        if n_voter == 200000:
            print(f"   üèÜ H100 simulated 200K voter election in {gpu_time:.2f} seconds!")

def run_gpu_multi_gpu_sync():
    """Run multi-GPU synchronization test"""
    print("\nüî• PHASE 5: GPU MULTI-GPU SYNCHRONIZATION")
    print("-" * 50)
    
    gpu_count = torch.cuda.device_count()
    print(f"\nüìä GPU: Testing {gpu_count} GPU synchronization")
    
    if gpu_count > 1:
        # Create tensors on each GPU
        tensors = []
        for i in range(gpu_count):
            with torch.cuda.device(i):
                tensors.append(torch.randn(1000, 1000, dtype=torch.float32, device='cuda'))
        
        # Synchronize all GPUs
        torch.cuda.synchronize()
        start = time.time()
        
        # Perform operations on all GPUs
        results = []
        for i, tensor in enumerate(tensors):
            with torch.cuda.device(i):
                result = torch.mm(tensor, tensor)
                results.append(result)
        
        torch.cuda.synchronize()
        total_time = time.time() - start
        
        gpu_times['multi_gpu_sync'] = total_time
        
        print(f"   ‚úÖ GPU: Synchronized {gpu_count} GPUs in {total_time:.3f}s")
        print(f"   üèÜ Multi-GPU matrix operations completed!")
    else:
        print("   ‚ÑπÔ∏è  Single GPU system - skipping multi-GPU demo")
        gpu_times['multi_gpu_sync'] = 0.0

def run_cpu_matrix_ops():
    """Run all matrix operations on CPU"""
    print("\nüî• PHASE 6: CPU MATRIX OPERATIONS")
    print("-" * 50)
    
    sizes = [8192, 16384, 32768]
    
    for size in sizes:
        print(f"\nüìä CPU: {size:,} √ó {size:,} matrix multiplication")
        print(f"   Matrix size: {size * size * 4 / 1e9:.1f} GB")
        
        # CPU test
        print("   üñ•Ô∏è  CPU calculation...")
        a_cpu = torch.randn(size, size, dtype=torch.float32)
        b_cpu = torch.randn(size, size, dtype=torch.float32)
        
        start = time.time()
        c_cpu = torch.mm(a_cpu, b_cpu)
        cpu_time = time.time() - start
        cpu_gflops = (2 * size**3) / cpu_time / 1e9
        
        cpu_times[f'matrix_{size}'] = cpu_time
        
        print(f"   ‚úÖ CPU: {cpu_time:.2f}s ({cpu_gflops:.0f} GFLOPS)")

def run_cpu_memory_bandwidth():
    """Run memory bandwidth tests on CPU"""
    print("\nüî• PHASE 7: CPU MEMORY BANDWIDTH")
    print("-" * 50)
    
    sizes_gb = [1, 4, 8, 16]
    
    for size_gb in sizes_gb:
        size_elements = int(size_gb * 1024**3 // 4)  # float32 = 4 bytes
        print(f"\nüìä CPU: {size_gb}GB memory copy")
        
        # Create data
        data_cpu = torch.randn(size_elements, dtype=torch.float32)
        
        # CPU copy
        start = time.time()
        data_cpu_copy = data_cpu.clone()
        cpu_time = time.time() - start
        cpu_bandwidth = size_gb / cpu_time
        
        cpu_times[f'memory_{size_gb}gb_copy'] = cpu_time
        
        print(f"   üñ•Ô∏è  CPU Copy: {cpu_time:.2f}s ({cpu_bandwidth:.1f} GB/s)")

def run_cpu_parallel_processing():
    """Run parallel processing tests on CPU"""
    print("\nüî• PHASE 8: CPU PARALLEL PROCESSING")
    print("-" * 50)
    
    sizes = [10_000_000, 50_000_000, 100_000_000]
    
    for size in sizes:
        print(f"\nüìä CPU: {size:,} element parallel computation")
        
        # CPU test
        print("   üñ•Ô∏è  CPU processing...")
        a_cpu = torch.randn(size, dtype=torch.float32)
        b_cpu = torch.randn(size, dtype=torch.float32)
        
        start = time.time()
        c_cpu = torch.sin(a_cpu) * torch.cos(b_cpu) + torch.sqrt(torch.abs(a_cpu))
        cpu_time = time.time() - start
        
        cpu_times[f'parallel_{size}'] = cpu_time
        
        print(f"   ‚úÖ CPU: {cpu_time:.2f}s")

def run_cpu_monte_carlo():
    """Run Monte Carlo econometrics on CPU"""
    print("\nüî• PHASE 9: CPU MONTE CARLO ECONOMETRICS")
    print("-" * 50)
    
    print("\nüìä CPU: Large-Scale Monte Carlo Studies for Economics Research")
    
    # Monte Carlo parameters
    n_simulations = [10000, 50000, 100000]
    n_assets = 100
    n_time_periods = 252  # One trading year
    
    for n_sim in n_simulations:
        print(f"\nüéØ CPU Monte Carlo Study: {n_sim:,} simulations")
        print(f"   Assets: {n_assets}, Time periods: {n_time_periods}")
        print(f"   Total calculations: {n_sim * n_assets * n_time_periods:,}")
        
        # CPU Monte Carlo - Asset Price Simulation
        print("   üñ•Ô∏è  CPU: Simulating asset price paths...")
        start = time.time()
        
        # Simulate asset returns with random walk
        cpu_returns = np.random.normal(0.0001, 0.02, (n_sim, n_assets, n_time_periods))
        cpu_prices = np.cumprod(1 + cpu_returns, axis=2)
        
        # Calculate portfolio statistics
        cpu_portfolio_values = np.sum(cpu_prices, axis=1)
        cpu_var_95 = np.percentile(cpu_portfolio_values, 5)
        cpu_expected_return = np.mean(cpu_portfolio_values)
        
        cpu_time = time.time() - start
        
        cpu_times[f'monte_carlo_{n_sim}'] = cpu_time
        
        print(f"   ‚úÖ CPU: {cpu_time:.2f}s (VaR: ${cpu_var_95:.2f}, E[R]: {cpu_expected_return:.4f})")
    
    # Bootstrap Confidence Intervals Demo
    print(f"\nüìä CPU Bootstrap Confidence Intervals (10,000 resamples)")
    print("   Simulating: Regression coefficient confidence intervals")
    
    # Generate synthetic economic data
    n_obs = 10000
    x = torch.randn(n_obs, 5)  # 5 explanatory variables
    true_beta = torch.tensor([1.5, -0.8, 0.3, 1.2, -0.5])
    noise = torch.normal(0, 0.1, (n_obs,))
    y = torch.matmul(x, true_beta) + noise
    
    # Bootstrap regression coefficients
    n_bootstrap = 10000
    bootstrap_betas = torch.zeros(n_bootstrap, 5)
    
    start = time.time()
    
    for i in range(n_bootstrap):
        # Bootstrap sample
        indices = torch.randint(0, n_obs, (n_obs,))
        x_boot = x[indices]
        y_boot = y[indices]
        
        # OLS regression: (X'X)^(-1) X'y
        xtx = torch.matmul(x_boot.t(), x_boot)
        xty = torch.matmul(x_boot.t(), y_boot)
        beta_hat = torch.linalg.solve(xtx, xty)
        
        bootstrap_betas[i] = beta_hat
    
    bootstrap_time = time.time() - start
    
    cpu_times['bootstrap_10000'] = bootstrap_time
    
    # Calculate confidence intervals
    ci_lower = torch.quantile(bootstrap_betas, 0.025, dim=0)
    ci_upper = torch.quantile(bootstrap_betas, 0.975, dim=0)
    
    print(f"   ‚úÖ CPU Bootstrap completed in {bootstrap_time:.2f} seconds")
    print(f"   üìà 95% Confidence Intervals:")
    for i in range(5):
        print(f"      Œ≤{i+1}: [{ci_lower[i]:.3f}, {ci_upper[i]:.3f}] (true: {true_beta[i]:.3f})")

def run_cpu_social_science_simulations():
    """Run social science simulations on CPU"""
    print("\nüî• PHASE 9.5: CPU SOCIAL SCIENCE SIMULATIONS")
    print("-" * 50)
    
    print("\nüìä CPU: Large-Scale Social Science Research Simulations")
    
    # Agent-Based Model: Social Contagion Simulation
    print("\nüß† CPU Agent-Based Model: Social Contagion Simulation")
    
    n_agents = [10000, 50000, 100000]
    n_iterations = 100
    
    for n_agent in n_agents:
        print(f"\nüéØ CPU Agent-Based Model: {n_agent:,} agents, {n_iterations} iterations")
        
        start = time.time()
        
        # Initialize agent states (0 = unaware, 1 = aware)
        agent_states = np.zeros(n_agent)
        agent_states[0] = 1.0  # Start with one informed agent
        
        # Social network connectivity (random connections)
        connectivity = np.random.rand(n_agent, n_agent) < 0.01  # 1% connection probability
        connectivity = connectivity.astype(np.float32)
        
        # Contagion simulation
        for iteration in range(n_iterations):
            # Calculate influence from neighbors
            influence = np.matmul(connectivity, agent_states)
            # Update states based on influence threshold
            new_states = np.where(influence > 0.5, 1.0, agent_states)
            agent_states = new_states
        
        cpu_time = time.time() - start
        
        final_adoption = np.sum(agent_states)
        adoption_rate = final_adoption / n_agent * 100
        
        cpu_times[f'agent_based_{n_agent}'] = cpu_time
        
        print(f"   ‚úÖ CPU: {cpu_time:.2f}s (Final adoption: {adoption_rate:.1f}%)")
    
    # Network Analysis: Community Detection
    print("\nüåê CPU Network Analysis: Community Detection")
    
    n_nodes = [5000, 10000, 20000]
    
    for n_node in n_nodes:
        print(f"\nüéØ CPU Network Analysis: {n_node:,} nodes")
        
        start = time.time()
        
        # Generate random network (sparse)
        edge_prob = 0.001  # Sparse network
        adjacency = np.random.rand(n_node, n_node) < edge_prob
        adjacency = adjacency.astype(np.float32)
        adjacency = np.triu(adjacency, k=1) + np.triu(adjacency, k=1).T  # Make symmetric
        
        # Calculate network metrics
        degree_centrality = np.sum(adjacency, axis=1)
        clustering_coeff = np.zeros(n_node)
        
        # Calculate clustering coefficient for each node
        for i in range(n_node):
            neighbors = np.where(adjacency[i] > 0)[0]
            if len(neighbors) > 1:
                neighbor_connections = adjacency[neighbors][:, neighbors]
                triangles = np.sum(neighbor_connections) / 2
                possible_triangles = len(neighbors) * (len(neighbors) - 1) / 2
                if possible_triangles > 0:
                    clustering_coeff[i] = triangles / possible_triangles
        
        avg_clustering = np.mean(clustering_coeff)
        avg_degree = np.mean(degree_centrality)
        
        cpu_time = time.time() - start
        
        cpu_times[f'network_{n_node}'] = cpu_time
        
        print(f"   ‚úÖ CPU: {cpu_time:.2f}s (Avg degree: {avg_degree:.1f}, Clustering: {avg_clustering:.3f})")
    
    # Survey Research: Large-Scale Factor Analysis
    print("\nüìä CPU Survey Research: Large-Scale Factor Analysis")
    
    n_respondents = [10000, 50000, 100000]
    n_items = 50
    n_factors = 5
    
    for n_resp in n_respondents:
        print(f"\nüéØ CPU Factor Analysis: {n_resp:,} respondents, {n_items} items, {n_factors} factors")
        
        start = time.time()
        
        # Generate survey data (responses 1-7 Likert scale)
        survey_data = np.random.randint(1, 8, (n_resp, n_items)).astype(np.float32)
        
        # Standardize data
        survey_std = np.std(survey_data, axis=0, keepdims=True)
        survey_mean = np.mean(survey_data, axis=0, keepdims=True)
        survey_z = (survey_data - survey_mean) / survey_std
        
        # Calculate correlation matrix
        corr_matrix = np.matmul(survey_z.T, survey_z) / (n_resp - 1)
        
        # Simple factor analysis (eigenvalue decomposition)
        eigenvalues, eigenvectors = np.linalg.eigh(corr_matrix)
        
        # Extract top factors
        top_eigenvalues = eigenvalues[-n_factors:]
        top_eigenvectors = eigenvectors[:, -n_factors:]
        
        # Calculate factor loadings
        factor_loadings = top_eigenvectors * np.sqrt(top_eigenvalues.reshape(1, -1))
        
        cpu_time = time.time() - start
        
        explained_variance = np.sum(top_eigenvalues) / np.sum(eigenvalues) * 100
        
        cpu_times[f'factor_analysis_{n_resp}'] = cpu_time
        
        print(f"   ‚úÖ CPU: {cpu_time:.2f}s (Explained variance: {explained_variance:.1f}%)")
    
    # Political Science: Voting Behavior Simulation
    print("\nüó≥Ô∏è CPU Political Science: Voting Behavior Simulation")
    
    n_voters = [50000, 100000, 200000]
    n_candidates = 5
    n_issues = 10
    
    for n_voter in n_voters:
        print(f"\nüéØ CPU Voting Simulation: {n_voter:,} voters, {n_candidates} candidates, {n_issues} issues")
        
        start = time.time()
        
        # Generate voter preferences on issues (1-10 scale)
        voter_preferences = np.random.rand(n_voter, n_issues) * 10
        
        # Generate candidate positions on issues
        candidate_positions = np.random.rand(n_candidates, n_issues) * 10
        
        # Calculate preference distances
        distances = np.zeros((n_voter, n_candidates))
        for i in range(n_candidates):
            diff = voter_preferences - candidate_positions[i].reshape(1, -1)
            distances[:, i] = np.sqrt(np.sum(diff**2, axis=1))
        
        # Convert distances to preferences (lower distance = higher preference)
        preferences = 1.0 / (1.0 + distances)
        
        # Simulate voting (proportional to preferences)
        vote_probabilities = preferences / np.sum(preferences, axis=1, keepdims=True)
        votes = np.array([np.random.choice(n_candidates, p=probs) for probs in vote_probabilities])
        
        # Calculate vote shares
        vote_counts = np.bincount(votes, minlength=n_candidates)
        vote_shares = vote_counts / n_voter * 100
        
        cpu_time = time.time() - start
        
        cpu_times[f'voting_{n_voter}'] = cpu_time
        
        print(f"   ‚úÖ CPU: {cpu_time:.2f}s (Winner: Candidate {np.argmax(vote_shares)}, {vote_shares[np.argmax(vote_shares)]:.1f}%)")

def print_comprehensive_summary():
    """Print comprehensive GPU vs CPU performance summary"""
    print("\n" + "=" * 80)
    print("üéâ COMPREHENSIVE PERFORMANCE SUMMARY")
    print("=" * 80)
    
    print("\nüìä GPU vs CPU Performance Comparison")
    print("-" * 50)
    
    # Matrix operations
    print("\nüî• MATRIX OPERATIONS:")
    for size in [8192, 16384, 32768]:
        gpu_time = gpu_times.get(f'matrix_{size}', 0)
        cpu_time = cpu_times.get(f'matrix_{size}', 0)
        if gpu_time > 0 and cpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"   {size:,}√ó{size:,}: GPU {gpu_time:.2f}s vs CPU {cpu_time:.2f}s = {speedup:.1f}x speedup")
    
    # Memory bandwidth
    print("\nüî• MEMORY BANDWIDTH:")
    for size_gb in [1, 4, 8, 16]:
        gpu_h2d = gpu_times.get(f'memory_{size_gb}gb_h2d', 0)
        gpu_d2h = gpu_times.get(f'memory_{size_gb}gb_d2h', 0)
        cpu_copy = cpu_times.get(f'memory_{size_gb}gb_copy', 0)
        if gpu_h2d > 0 and cpu_copy > 0:
            h2d_bandwidth = size_gb / gpu_h2d
            d2h_bandwidth = size_gb / gpu_d2h
            cpu_bandwidth = size_gb / cpu_copy
            print(f"   {size_gb}GB: GPU H2D {h2d_bandwidth:.1f}GB/s, D2H {d2h_bandwidth:.1f}GB/s vs CPU {cpu_bandwidth:.1f}GB/s")
    
    # Parallel processing
    print("\nüî• PARALLEL PROCESSING:")
    for size in [10_000_000, 50_000_000, 100_000_000]:
        gpu_time = gpu_times.get(f'parallel_{size}', 0)
        cpu_time = cpu_times.get(f'parallel_{size}', 0)
        if gpu_time > 0 and cpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"   {size:,} elements: GPU {gpu_time:.2f}s vs CPU {cpu_time:.2f}s = {speedup:.1f}x speedup")
    
    # Monte Carlo
    print("\nüî• MONTE CARLO ECONOMETRICS:")
    for n_sim in [10000, 50000, 100000]:
        gpu_time = gpu_times.get(f'monte_carlo_{n_sim}', 0)
        cpu_time = cpu_times.get(f'monte_carlo_{n_sim}', 0)
        if gpu_time > 0 and cpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"   {n_sim:,} simulations: GPU {gpu_time:.2f}s vs CPU {cpu_time:.2f}s = {speedup:.1f}x speedup")
    
    # Bootstrap
    gpu_bootstrap = gpu_times.get('bootstrap_10000', 0)
    cpu_bootstrap = cpu_times.get('bootstrap_10000', 0)
    if gpu_bootstrap > 0 and cpu_bootstrap > 0:
        bootstrap_speedup = cpu_bootstrap / gpu_bootstrap
        print(f"   10,000 bootstrap: GPU {gpu_bootstrap:.2f}s vs CPU {cpu_bootstrap:.2f}s = {bootstrap_speedup:.1f}x speedup")
    
    # Social Science Simulations
    print("\nüî• SOCIAL SCIENCE SIMULATIONS:")
    
    # Agent-based models
    for n_agent in [10000, 50000, 100000]:
        gpu_time = gpu_times.get(f'agent_based_{n_agent}', 0)
        cpu_time = cpu_times.get(f'agent_based_{n_agent}', 0)
        if gpu_time > 0 and cpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"   {n_agent:,} agents: GPU {gpu_time:.2f}s vs CPU {cpu_time:.2f}s = {speedup:.1f}x speedup")
    
    # Network analysis
    for n_node in [5000, 10000, 20000]:
        gpu_time = gpu_times.get(f'network_{n_node}', 0)
        cpu_time = cpu_times.get(f'network_{n_node}', 0)
        if gpu_time > 0 and cpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"   {n_node:,} nodes: GPU {gpu_time:.2f}s vs CPU {cpu_time:.2f}s = {speedup:.1f}x speedup")
    
    # Factor analysis
    for n_resp in [10000, 50000, 100000]:
        gpu_time = gpu_times.get(f'factor_analysis_{n_resp}', 0)
        cpu_time = cpu_times.get(f'factor_analysis_{n_resp}', 0)
        if gpu_time > 0 and cpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"   {n_resp:,} respondents: GPU {gpu_time:.2f}s vs CPU {cpu_time:.2f}s = {speedup:.1f}x speedup")
    
    # Voting simulation
    for n_voter in [50000, 100000, 200000]:
        gpu_time = gpu_times.get(f'voting_{n_voter}', 0)
        cpu_time = cpu_times.get(f'voting_{n_voter}', 0)
        if gpu_time > 0 and cpu_time > 0:
            speedup = cpu_time / gpu_time
            print(f"   {n_voter:,} voters: GPU {gpu_time:.2f}s vs CPU {cpu_time:.2f}s = {speedup:.1f}x speedup")
    
    # Overall summary
    print("\nüèÜ KEY TAKEAWAYS:")
    print("‚Ä¢ H100 can process massive matrices in seconds")
    print("‚Ä¢ Memory bandwidth exceeds 10 GB/s")
    print("‚Ä¢ 50-100x speedup on parallel workloads")
    print("‚Ä¢ Real-time processing of millions of elements")
    print("‚Ä¢ Monte Carlo studies: 100K simulations in seconds")
    print("‚Ä¢ Bootstrap methods: 10K resamples for robust inference")
    print("‚Ä¢ Agent-based models: 100K agents in seconds")
    print("‚Ä¢ Network analysis: 20K node networks in seconds")
    print("‚Ä¢ Survey research: 100K respondent factor analysis")
    print("‚Ä¢ Political science: 200K voter simulations")
    print("‚Ä¢ GPU computing transforms social science research workflows")
    print("=" * 80)

def main():
    print_header()
    
    # Check if CUDA is available
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available. Cannot run H100 demonstrations.")
        return
    
    print(f"‚úÖ CUDA available: {torch.cuda.device_count()} GPU(s)")
    
    # Phase 1-5: Run all GPU tests first
    run_gpu_matrix_ops()
    run_gpu_memory_bandwidth()
    run_gpu_parallel_processing()
    run_gpu_monte_carlo()
    run_gpu_social_science_simulations()
    run_gpu_multi_gpu_sync()
    
    # Phase 6-9: Run all CPU tests
    run_cpu_matrix_ops()
    run_cpu_memory_bandwidth()
    run_cpu_parallel_processing()
    run_cpu_monte_carlo()
    run_cpu_social_science_simulations()
    
    # Final comprehensive summary
    print_comprehensive_summary()
    
    # Optional: Run real-time monitoring
    response = input("\nWould you like to see real-time GPU monitoring? (y/n): ")
    if response.lower() == 'y':
        print("\nüî• DEMO 10: REAL-TIME PERFORMANCE METRICS")
        print("-" * 50)
        
        print("\nüìä Live GPU utilization monitoring...")
        print("   Press Ctrl+C to stop monitoring")
        
        try:
            while True:
                # Get GPU memory info
                memory_allocated = torch.cuda.memory_allocated(0) / 1e9
                memory_reserved = torch.cuda.memory_reserved(0) / 1e9
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
                
                # Get GPU utilization (if available)
                try:
                    import pynvml
                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                    utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    gpu_util = utilization.gpu
                    memory_util = utilization.memory
                except:
                    gpu_util = "N/A"
                    memory_util = "N/A"
                
                print(f"\r   üöÄ GPU: {gpu_util}% | Memory: {memory_allocated:.1f}GB/{memory_total:.1f}GB | Reserved: {memory_reserved:.1f}GB", end="")
                time.sleep(1)
                
        except KeyboardInterrupt:
            print("\n   ‚úÖ Monitoring stopped")

if __name__ == "__main__":
    main()
